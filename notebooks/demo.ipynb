{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/miniconda3/envs/massspecgym/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from massspecgym.data import MassSpecDataset, RetrievalDataset, MassSpecDataModule\n",
    "from massspecgym.transforms import SpecTokenizer, MolFingerprinter\n",
    "from massspecgym.models.retrieval import DeepSetsRetrieval, RandomRetrieval\n",
    "from massspecgym.models.de_novo import DummyDeNovo\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    mgf_pth = \"../data/debug/example_5_spectra.mgf\"\n",
    "    candidates_pth = \"../data/debug/example_5_spectra_candidates.json\"\n",
    "    split_pth=\"../data/debug/example_5_spectra_split.tsv\"\n",
    "else:\n",
    "    # Use default benchmark paths\n",
    "    mgf_pth = None\n",
    "    candidates_pth = None\n",
    "    split_pth = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Sets model on the fingerprint retrieval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/anton/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# Uncomment the paths to use debugging data containing only 5 spectra\n",
    "dataset = RetrievalDataset(\n",
    "    mgf_pth=mgf_pth,\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform=MolFingerprinter(),\n",
    "    candidates_pth=candidates_pth,\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = DeepSetsRetrieval()\n",
    "# model = RandomRetrieval()\n",
    "\n",
    "# Init logger\n",
    "# You may need to run wandb init first to use the wandb logger\n",
    "# Alternatively set logger = None in Trainer below not to use wandb\n",
    "project = \"MassSpecGymRetrieval\"\n",
    "name = \"DeepSets\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=50, logger=logger, log_every_n_steps=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy model on the de novo generation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/anton/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# Uncomment the paths to use debugging data containing only 5 spectra\n",
    "dataset = MassSpecDataset(\n",
    "    mgf_pth=mgf_pth,\n",
    "    spec_transform=SpecTokenizer(n_peaks=60),\n",
    "    mol_transform=None\n",
    ")\n",
    "\n",
    "# Init data module\n",
    "data_module = MassSpecDataModule(\n",
    "    dataset=dataset,\n",
    "    split_pth=split_pth,\n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "# Init model\n",
    "model = DummyDeNovo()\n",
    "\n",
    "# Init logger\n",
    "# You may need to run wandb init first to use the wandb logger\n",
    "# Alternatively set logger = None in Trainer below not to use wandb\n",
    "project = \"MassSpecGymDeNovo\"\n",
    "name = \"DummyBasline\"\n",
    "logger = pl.loggers.WandbLogger(\n",
    "    project=project,\n",
    "    name=name,\n",
    "    tags=[],\n",
    "    log_model=False,\n",
    ")\n",
    "\n",
    "# Init trainer\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\", max_epochs=50, logger=logger, log_every_n_steps=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:181: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "\n",
      "  | Name                      | Type       | Params\n",
      "---------------------------------------------------------\n",
      "0 | val_top_1_min_mces_dist   | MeanMetric | 0     \n",
      "1 | val_top_10_min_mces_dist  | MeanMetric | 0     \n",
      "2 | train_top_1_min_mces_dist | MeanMetric | 0     \n",
      "---------------------------------------------------------\n",
      "0         Trainable params\n",
      "0         Non-trainable params\n",
      "0         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 1/? [00:00<00:00, 301.31it/s]                 mols_pred [['CN1C=NC2=C1C(=O)N(C(=O)N2C)C', 'CC(=O)Oc1ccccc1C(=O)O', 'c1ccccc1', 'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O', 'CC(=O)Oc1ccccc1C(=O)O', 'C', 'CC(=O)O', 'CC(=O)C', 'CC(C)CC1=CC=C(C=C1)C(C)C(=O)O', 'CC(=O)Oc1ccccc1C(=O)O']]\n",
      "mol_label ['COc1nc(N2CCC3(CCCN(Cc4c[nH]c5ccccc45)C3=O)CC2)ncc1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anton/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "Python argument types in\n    rdkit.Chem.rdmolfiles.MolToSmiles(str)\ndid not match C++ signature:\n    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdata_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1030\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1032\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1033\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1057\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1059\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1062\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:411\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    409\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, hook_name, output, \u001b[39m*\u001b[39mhook_kwargs\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m--> 411\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(trainer, hook_name, output, \u001b[39m*\u001b[39;49mhook_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    413\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_batch_end()\n\u001b[1;32m    415\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/miniconda3/envs/massspecgym/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/dev/ms/MassSpecGym/massspecgym/models/base.py:50\u001b[0m, in \u001b[0;36mMassSpecGymModel.on_validation_batch_end\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_validation_batch_end\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_batch_end(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, metric_pref\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mval_\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/dev/ms/MassSpecGym/massspecgym/models/de_novo/base.py:43\u001b[0m, in \u001b[0;36mDeNovoMassSpecGymModel.on_batch_end\u001b[0;34m(self, outputs, batch, batch_idx, metric_pref)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_batch_end\u001b[39m(\n\u001b[1;32m     37\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     38\u001b[0m     outputs: T\u001b[39m.\u001b[39mAny,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     metric_pref: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     42\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_de_novo_step(\n\u001b[1;32m     44\u001b[0m         outputs[\u001b[39m\"\u001b[39;49m\u001b[39mmols_pred\u001b[39;49m\u001b[39m\"\u001b[39;49m],  \u001b[39m# (bs, k) list of generated rdkit molecules or SMILES strings\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m         batch[\u001b[39m\"\u001b[39;49m\u001b[39mmol\u001b[39;49m\u001b[39m\"\u001b[39;49m],  \u001b[39m# (bs) list of ground truth SMILES strings\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m         metric_pref\u001b[39m=\u001b[39;49mmetric_pref\n\u001b[1;32m     47\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/ms/MassSpecGym/massspecgym/models/de_novo/base.py:68\u001b[0m, in \u001b[0;36mDeNovoMassSpecGymModel.evaluate_de_novo_step\u001b[0;34m(self, mols_pred, mol_label, metric_pref)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m# Convert SMILEs from Mol objects if needed\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(mols_pred[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> 68\u001b[0m     mols_pred \u001b[39m=\u001b[39m [Chem\u001b[39m.\u001b[39;49mMolToSmiles(m) \u001b[39mfor\u001b[39;49;00m ms \u001b[39min\u001b[39;49;00m mols_pred \u001b[39mfor\u001b[39;49;00m m \u001b[39min\u001b[39;49;00m ms]\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m top_k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_ks:\n\u001b[1;32m     71\u001b[0m     \u001b[39m# 1. Evaluate minimum common edge subgraph:\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39m# Calculate MCES distance between top-k predicted molecules and ground truth and\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[39m# report the minimum distance. The minimum distances for each sample in the batch are\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[39m# averaged across the epoch.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     min_mces_dists \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/dev/ms/MassSpecGym/massspecgym/models/de_novo/base.py:68\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m# Convert SMILEs from Mol objects if needed\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(mols_pred[\u001b[39m0\u001b[39m], \u001b[39mstr\u001b[39m):\n\u001b[0;32m---> 68\u001b[0m     mols_pred \u001b[39m=\u001b[39m [Chem\u001b[39m.\u001b[39;49mMolToSmiles(m) \u001b[39mfor\u001b[39;00m ms \u001b[39min\u001b[39;00m mols_pred \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m ms]\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m top_k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_ks:\n\u001b[1;32m     71\u001b[0m     \u001b[39m# 1. Evaluate minimum common edge subgraph:\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39m# Calculate MCES distance between top-k predicted molecules and ground truth and\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[39m# report the minimum distance. The minimum distances for each sample in the batch are\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[39m# averaged across the epoch.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     min_mces_dists \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    rdkit.Chem.rdmolfiles.MolToSmiles(str)\ndid not match C++ signature:\n    MolToSmiles(RDKit::ROMol mol, bool isomericSmiles=True, bool kekuleSmiles=False, int rootedAtAtom=-1, bool canonical=True, bool allBondsExplicit=False, bool allHsExplicit=False, bool doRandom=False)\n    MolToSmiles(RDKit::ROMol mol, RDKit::SmilesWriteParams params)"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "massspecgym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
